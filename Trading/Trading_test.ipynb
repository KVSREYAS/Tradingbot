{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_network(nn.Module):\n",
    "    def __init__(self,lr,input_dims,output_dims):\n",
    "        super(DQN_network,self).__init__()\n",
    "        self.fc_1=nn.Linear(*input_dims,128)\n",
    "        self.fc_2=nn.Linear(128,256)\n",
    "        self.fc_3=nn.Linear(256,256)\n",
    "        self.fc_4=nn.Linear(256,128)\n",
    "        self.fc_5=nn.Linear(128,output_dims)\n",
    "        \n",
    "        self.lr=lr\n",
    "        self.loss=nn.HuberLoss()\n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x=F.relu(self.fc_1(state))\n",
    "        x=F.relu(self.fc_2(x))\n",
    "        x=F.relu(self.fc_3(x))\n",
    "        x=F.relu(self.fc_4(x))\n",
    "        actions=self.fc_5(x)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,strategy=\"t-dqn\",reset_every=1000,pretrained=False,model_name=None):\n",
    "        self.strategy=strategy\n",
    "        \n",
    "        self.state_size=state_size\n",
    "        self.action_size=3\n",
    "        self.model_name=model_name\n",
    "        self.inventory=[]\n",
    "        self.memory=deque(maxlen=10000)\n",
    "        self.first_iter=True\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model_name=model_name\n",
    "        self.gamma=0.95\n",
    "        self.epsilon=1\n",
    "        self.epsilon_decay=0.995\n",
    "        self.epsilon_min=0.01\n",
    "        self.learning_rate=0.001\n",
    "        self.loss=nn.HuberLoss()\n",
    "        self.custom_objects={\"huber_loss\":nn.HuberLoss()}\n",
    "        \n",
    "        \n",
    "        if pretrained and self.model_name is not None:\n",
    "            self.model=self.load()\n",
    "        else:\n",
    "            self.model=DQN_network(lr=self.learning_rate,input_dims=self.state_size,output_dims=self.action_size)\n",
    "            \n",
    "        if self.strategy in [\"t-dqn\",\"double-dqn\"]:\n",
    "            self.n_iter=1\n",
    "            self.reset_every=reset_every\n",
    "            \n",
    "            self.target_model=copy.deepcopy(self.model)\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state,is_eval=False):\n",
    "        if not is_eval and random.random() <=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        if self.first_iter:\n",
    "            self.first_iter=False\n",
    "            return 1\n",
    "        state=T.tensor(state,dtype=T.float32)\n",
    "        device=T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "        action_probs=T.FloatTensor(state).to(device)\n",
    "        return np.argmax(action_probs)\n",
    "    \n",
    "    def train_exp_relay(self,batch_size):\n",
    "        batch=random.sample(self.memory,batch_size)\n",
    "        X_train,y_train = [],[]\n",
    "        states,actions,rewards,next_states,dones=zip(*batch)\n",
    "        \n",
    "        \n",
    "        device=T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "        states = T.FloatTensor(states).to(device)\n",
    "        next_states = T.FloatTensor(next_states).to(device)\n",
    "        rewards = T.FloatTensor(rewards).to(device)\n",
    "        dones = T.FloatTensor(dones).to(device)\n",
    "        \n",
    "        if self.strategy==\"dqn\":\n",
    "            with T.no_grad():\n",
    "                max_next_q_values=self.model.forward(next_states).max(dim=1)[0]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "            \n",
    "        elif self.strategy==\"t-dqn\":\n",
    "            if self.n_iter%self.reset_every==0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            with T.no_grad():\n",
    "                max_next_q_values=self.target_model.forward(next_states).max(dim=1)[0]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "        elif self.strategy==\"double=dqn\":\n",
    "            if self.n_iter%self.reset_every==0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            with T.no_grad():\n",
    "                next_q_value=np.argmax(self.model.forward(next_states))\n",
    "                max_next_q_values=self.target_model.forward(next_states)[next_q_value]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        batch_index=np.arange(batch_size,dtype=np.int32)\n",
    "        predicted_vals=self.model.forward(states)[batch_index,actions]\n",
    "        \n",
    "        self.model.optimizer.zero_grad()\n",
    "    \n",
    "        loss=self.loss(predicted_vals,target_values)\n",
    "        loss.backward()\n",
    "        self.model.optimizer.step()\n",
    "        \n",
    "        if self.epsilon>self.epsilon_min:\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "            \n",
    "        return loss\n",
    "                \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(random.randrange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 2.27\n",
      "Episode 2: Total Reward: 2.96\n",
      "Episode 3: Total Reward: 0.80\n",
      "Episode 4: Total Reward: 1.79\n",
      "Episode 5: Total Reward: 1.64\n",
      "Episode 6: Total Reward: 6.43\n",
      "Episode 7: Total Reward: 0.80\n",
      "Episode 8: Total Reward: 6.42\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[120], line 35\u001b[0m, in \u001b[0;36mtest_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 35\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     37\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[119], line 48\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self, state, is_eval)\u001b[0m\n\u001b[0;32m     46\u001b[0m device\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m T\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m action_probs\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:68\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Mock Environment\n",
    "class MockEnvironment:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_space = 3  # For simplicity, three actions\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.rand(self.state_size)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = np.random.rand(self.state_size)\n",
    "        reward = random.random()  # Random reward\n",
    "        done = random.random() < 0.1  # 10% chance to finish the episode\n",
    "        return next_state, reward, done\n",
    "\n",
    "# Test Program\n",
    "def test_agent():\n",
    "    state_size = 4  # Example state size\n",
    "    agent = Agent(state_size=[state_size], strategy=\"t-dqn\")\n",
    "    env = MockEnvironment(state_size)\n",
    "\n",
    "    episodes = 10\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train the agent with a small batch size\n",
    "            if len(agent.memory) > 32:  # Ensure there's enough memory to sample from\n",
    "                agent.train_exp_relay(batch_size=32)\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Run the test\n",
    "test_agent()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
