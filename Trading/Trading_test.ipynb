{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_network(nn.Module):\n",
    "    def __init__(self,lr,input_dims,output_dims):\n",
    "        super(DQN_network,self).__init__()\n",
    "        self.fc_1=nn.Linear(*input_dims,128)\n",
    "        self.fc_2=nn.Linear(128,256)\n",
    "        self.fc_3=nn.Linear(256,256)\n",
    "        self.fc_4=nn.Linear(256,128)\n",
    "        self.fc_5=nn.Linear(128,output_dims)\n",
    "        \n",
    "        self.lr=lr\n",
    "        self.loss=nn.HuberLoss()\n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x=F.relu(self.fc_1(state))\n",
    "        x=F.relu(self.fc_2(x))\n",
    "        x=F.relu(self.fc_3(x))\n",
    "        x=F.relu(self.fc_4(x))\n",
    "        actions=self.fc_5(x)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,strategy=\"t-dqn\",reset_every=1000,pretrained=False,model_name=None,epsilon_decay=0.995):\n",
    "        self.strategy=strategy\n",
    "        \n",
    "        self.state_size=state_size\n",
    "        self.action_size=3\n",
    "        self.model_name=model_name\n",
    "        self.inventory=[]\n",
    "        self.memory=deque(maxlen=10000)\n",
    "        self.first_iter=True\n",
    "        self.loss_val=0\n",
    "        \n",
    "        \n",
    "        self.model_name=model_name\n",
    "        self.gamma=0.95\n",
    "        self.epsilon=1\n",
    "        self.epsilon_decay=epsilon_decay\n",
    "        self.epsilon_min=0.01\n",
    "        self.learning_rate=0.001\n",
    "        self.loss=nn.HuberLoss()\n",
    "        self.custom_objects={\"huber_loss\":nn.HuberLoss()}\n",
    "        \n",
    "        \n",
    "        if pretrained and self.model_name is not None:\n",
    "            self.model=self.load()\n",
    "        else:\n",
    "            self.model=DQN_network(lr=self.learning_rate,input_dims=self.state_size,output_dims=self.action_size)\n",
    "            \n",
    "        if self.strategy in [\"t-dqn\",\"double-dqn\"]:\n",
    "            self.n_iter=1\n",
    "            self.reset_every=reset_every\n",
    "            \n",
    "            self.target_model=copy.deepcopy(self.model)\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state,is_eval=False):\n",
    "        if not is_eval and random.random() <=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        if self.first_iter:\n",
    "            self.first_iter=False\n",
    "            return 1\n",
    "        device=T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "        state_tensor=T.FloatTensor(state).to(device)\n",
    "        action_probs=self.model.forward(state_tensor)\n",
    "        return T.argmax(action_probs).item()\n",
    "    \n",
    "    def train_exp_relay(self,batch_size):\n",
    "        batch=random.sample(self.memory,batch_size)\n",
    "        X_train,y_train = [],[]\n",
    "        states,actions,rewards,next_states,dones=zip(*batch)\n",
    "        \n",
    "        batch_index=np.arange(batch_size,dtype=np.int32)\n",
    "        device=T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "        states = T.FloatTensor(states).to(device)\n",
    "        next_states = T.FloatTensor(next_states).to(device)\n",
    "        rewards = T.FloatTensor(rewards).to(device)\n",
    "        dones = T.FloatTensor(dones).to(device)\n",
    "        \n",
    "        if self.strategy==\"dqn\":\n",
    "            with T.no_grad():\n",
    "                max_next_q_values=self.model.forward(next_states).max(dim=1)[0]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "            \n",
    "        elif self.strategy==\"t-dqn\":\n",
    "            if self.n_iter%self.reset_every==0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            with T.no_grad():\n",
    "                max_next_q_values=self.target_model.forward(next_states).max(dim=1)[0]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "        elif self.strategy==\"double-dqn\":\n",
    "            if self.n_iter%self.reset_every==0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "            with T.no_grad():\n",
    "                action=T.argmax(self.model.forward(next_states),dim=1)\n",
    "                max_next_q_values=self.target_model.forward(next_states)[batch_index,action]\n",
    "            max_next_q_values[dones==1]=0.0\n",
    "            target_values=rewards+self.gamma*max_next_q_values\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "\n",
    "        predicted_vals=self.model.forward(states)[batch_index,actions]\n",
    "        \n",
    "        self.model.optimizer.zero_grad()\n",
    "    \n",
    "        loss=self.loss(predicted_vals,target_values)\n",
    "        self.loss_val=loss\n",
    "        loss.backward()\n",
    "        self.model.optimizer.step()\n",
    "        \n",
    "        if self.epsilon>self.epsilon_min:\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "            \n",
    "        return loss\n",
    "                \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(random.randrange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index=np.arange(32,dtype=np.int32)\n",
    "tensor = T.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "a=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 9])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(T.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv:\n",
    "    def __init__(self,data,initial_balance=10000):\n",
    "        self.data=data\n",
    "        self.initial_balance=initial_balance\n",
    "        self.current_step=0\n",
    "        self.balance=initial_balance\n",
    "        self.holdings=0\n",
    "        self.net_worth=initial_balance\n",
    "        self.max_steps=len(data)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step=0\n",
    "        self.balance=self.initial_balance\n",
    "        self.holdings=0\n",
    "        self.net_worth=self.initial_balance\n",
    "        state=self.data.iloc[self.current_step].values[1:]\n",
    "        return list(state)\n",
    "    \n",
    "    def get_state(self):\n",
    "        state=self.data.iloc[self.current_step].values[1:]\n",
    "        return list(state)\n",
    "    \n",
    "    def step(self,action):\n",
    "        \n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        self.current_price=current_price\n",
    "        reward=0\n",
    "        if action==1:#buy\n",
    "            if self.balance>current_price:\n",
    "                self.previous_balance=self.balance\n",
    "                self.holdings+=self.balance/current_price\n",
    "                self.balance=0\n",
    "                \n",
    "        elif action==2:#sell\n",
    "            if self.holdings>0:\n",
    "                self.balance+=self.holdings*current_price\n",
    "                self.holdings=0\n",
    "                \n",
    "                self.net_worth=self.balance+self.holdings*current_price\n",
    "                reward=self.net_worth-self.initial_balance\n",
    "        \n",
    "        self.current_step+=1\n",
    "        done=self.current_step>=self.max_steps-1\n",
    "        \n",
    "        return self.get_state(),reward,done,{}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 85090.97444939101,avg_reward: 85090.97444939101, Epsilon:0.6168553087570796,Loss:302167.125,Money earned:16665.08961023402\n",
      "Episode: 2, Total Reward: 220038.503922344,avg_reward: 152564.7391858675, Epsilon:0.37446925262555913,Loss:275048.4375,Money earned:18929.92473126836\n",
      "Episode: 3, Total Reward: 160457.13632224407,avg_reward: 155195.53823132635, Epsilon:0.22732595338198971,Loss:296950.375,Money earned:20321.91637342433\n",
      "Episode: 4, Total Reward: 49802.836210972135,avg_reward: 128847.3627262378, Epsilon:0.13800088717218023,Loss:227507.6875,Money earned:16731.39750207785\n",
      "Episode: 5, Total Reward: 46727.46154194088,avg_reward: 112423.38248937842, Epsilon:0.08377505769571193,Loss:159605.046875,Money earned:15124.928354663909\n",
      "Episode: 6, Total Reward: 32746.583322885926,avg_reward: 99143.91596162967, Epsilon:0.05085663169080453,Loss:167078.9375,Money earned:18221.0442022839\n",
      "Episode: 7, Total Reward: -1766.6617585586646,avg_reward: 84728.11914445991, Epsilon:0.030873114958970983,Loss:95199.140625,Money earned:11032.602653944801\n",
      "Episode: 8, Total Reward: 8109.092247905426,avg_reward: 75150.7407823906, Epsilon:0.018741886664157072,Loss:119221.265625,Money earned:13614.616500043881\n",
      "Episode: 9, Total Reward: 4696.582848537126,avg_reward: 67322.50101196243, Epsilon:0.011377482194424365,Loss:154472.40625,Money earned:13165.575301758841\n",
      "Episode: 10, Total Reward: 0,avg_reward: 60590.25091076619, Epsilon:0.009995187929535779,Loss:99545.1796875,Money earned:10000.0\n",
      "Episode: 11, Total Reward: -1644.9691316609587,avg_reward: 54932.5036341819, Epsilon:0.009995187929535779,Loss:183206.21875,Money earned:12015.020885097289\n",
      "Episode: 12, Total Reward: 4407.906187959452,avg_reward: 50722.120513663365, Epsilon:0.009995187929535779,Loss:69016.1796875,Money earned:18115.3984698588\n",
      "Episode: 13, Total Reward: 22387.722482994097,avg_reward: 48542.551434381116, Epsilon:0.009995187929535779,Loss:25781.337890625,Money earned:20078.726325282798\n",
      "Episode: 14, Total Reward: 13170.841202289463,avg_reward: 46016.00070351742, Epsilon:0.009995187929535779,Loss:408353.4375,Money earned:24703.755266768767\n",
      "Episode: 15, Total Reward: 15534.824356651718,avg_reward: 43983.922280393046, Epsilon:0.009995187929535779,Loss:350250.5625,Money earned:20924.022809317667\n",
      "Episode: 16, Total Reward: 2326.030448324089,avg_reward: 41380.30404088873, Epsilon:0.009995187929535779,Loss:80700.71875,Money earned:23322.90817699362\n",
      "Episode: 17, Total Reward: 30926.25732217965,avg_reward: 40765.36011625879, Epsilon:0.009995187929535779,Loss:340253.0625,Money earned:27834.35138175967\n",
      "Episode: 18, Total Reward: 5612.086230263772,avg_reward: 38812.40045592573, Epsilon:0.009995187929535779,Loss:290459.4375,Money earned:24463.173518719854\n",
      "Episode: 19, Total Reward: 20016.58658734545,avg_reward: 37823.1470944215, Epsilon:0.009995187929535779,Loss:333182.5,Money earned:23180.614388181006\n",
      "Episode: 20, Total Reward: 9238.39267390195,avg_reward: 36393.90937339553, Epsilon:0.009995187929535779,Loss:108305.15625,Money earned:22591.055287351355\n",
      "Episode: 21, Total Reward: 11220.574434414142,avg_reward: 35195.17913820594, Epsilon:0.009995187929535779,Loss:308848.03125,Money earned:24851.13497531797\n",
      "Episode: 22, Total Reward: 3524.4073099332054,avg_reward: 33755.59860055718, Epsilon:0.009995187929535779,Loss:46114.85546875,Money earned:12611.811123129435\n",
      "Episode: 23, Total Reward: 10849.420126490473,avg_reward: 32759.67779733689, Epsilon:0.009995187929535779,Loss:45678.9765625,Money earned:19198.277000188144\n",
      "Episode: 24, Total Reward: 42771.673111951066,avg_reward: 33176.84426877915, Epsilon:0.009995187929535779,Loss:32704.015625,Money earned:13589.713266496456\n",
      "Episode: 25, Total Reward: 11631.896758248578,avg_reward: 32315.046368357926, Epsilon:0.009995187929535779,Loss:28527.796875,Money earned:12894.055215117129\n",
      "Episode: 26, Total Reward: 7898.951093071668,avg_reward: 31375.965780846916, Epsilon:0.009995187929535779,Loss:30795.61328125,Money earned:12462.278607888535\n",
      "Episode: 27, Total Reward: -220.96109074288506,avg_reward: 30205.709230047298, Epsilon:0.009995187929535779,Loss:29081.8671875,Money earned:10939.848989319147\n",
      "Episode: 28, Total Reward: 7260.277990067158,avg_reward: 29386.22954290515, Epsilon:0.009995187929535779,Loss:28615.76953125,Money earned:11668.357157986386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory)\u001b[38;5;241m>\u001b[39mbatch_size:\n\u001b[1;32m---> 25\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_exp_relay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m reward_history\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m100\u001b[39m:\n",
      "Cell \u001b[1;32mIn[124], line 82\u001b[0m, in \u001b[0;36mAgent.train_exp_relay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m T\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     81\u001b[0m     action\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(next_states),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     max_next_q_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m[batch_index,action]\n\u001b[0;32m     83\u001b[0m max_next_q_values[dones\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     84\u001b[0m target_values\u001b[38;5;241m=\u001b[39mrewards\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mmax_next_q_values\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mDQN_network.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,state):\n\u001b[0;32m     18\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_1(state))\n\u001b[1;32m---> 19\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_3(x))\n\u001b[0;32m     21\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_4(x))\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sudhe\\RLwithpytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('MSFT.csv')\n",
    "data=data[-1000:]\n",
    "episodes=100\n",
    "batch_size=32\n",
    "env=StockTradingEnv(data)\n",
    "agent=Agent(state_size=[6],epsilon_decay=0.9995,strategy=\"double-dqn\")\n",
    "reward_history=[]\n",
    "for episode in range(episodes):\n",
    "    state=env.reset()\n",
    "    total_reward=0\n",
    "    while True:\n",
    "        action=agent.act(state)\n",
    "        next_state,reward,done,info=env.step(action)\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        \n",
    "        state=next_state\n",
    "        total_reward+=reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.train_exp_relay(batch_size)\n",
    "    reward_history.append(total_reward)\n",
    "    if episode>100:\n",
    "        avg_reward=sum(reward_history[-100:])/100\n",
    "    else:\n",
    "        avg_reward=sum(reward_history[-100:])/(episode+1)\n",
    "    print(f'Episode: {episode+1}, Total Reward: {total_reward},avg_reward: {avg_reward}, Epsilon:{agent.epsilon},Loss:{agent.loss_val},Money earned:{env.balance+env.holdings*env.current_price}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.49"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('MSFT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=StockTradingEnv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408.32993058391185\n"
     ]
    }
   ],
   "source": [
    "print(env.holdings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.data.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.step(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.709999, 26.08, 25.610001, 25.84, 21.241688, 59514000]\n"
     ]
    }
   ],
   "source": [
    "print(list(state[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
